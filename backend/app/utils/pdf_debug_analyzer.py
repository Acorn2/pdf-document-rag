import fitz  # PyMuPDF
import os
import json
import logging
from typing import Dict, List, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class PDFDebugAnalyzer:
    """PDFå†…å®¹æå–è°ƒè¯•åˆ†æå™¨ - ä¸“é—¨ç”¨äºæ’æŸ¥PyMuPDFæå–é—®é¢˜"""
    
    def __init__(self):
        self.debug_results = {}
    
    def comprehensive_analysis(self, file_path: str, max_pages: int = 20) -> Dict[str, Any]:
        """å…¨é¢åˆ†æPDFæ–‡ä»¶çš„å†…å®¹æå–æƒ…å†µ
        
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            max_pages: æœ€å¤§åˆ†æé¡µæ•°ï¼Œé»˜è®¤20é¡µï¼ˆ0è¡¨ç¤ºåˆ†ææ‰€æœ‰é¡µé¢ï¼‰
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹åˆ†æPDFæ–‡ä»¶: {os.path.basename(file_path)}")
        if max_pages > 0:
            print(f"åˆ†æèŒƒå›´: å‰{max_pages}é¡µ")
        else:
            print(f"åˆ†æèŒƒå›´: å…¨éƒ¨é¡µé¢")
        print(f"{'='*60}")
        
        results = {
            "file_info": {},
            "basic_extraction": {},
            "method_comparison": {},
            "page_analysis": [],
            "page_contents": [],  # æ–°å¢ï¼šè¯¦ç»†çš„é¡µé¢å†…å®¹
            "extracted_texts": {},  # æ–°å¢ï¼šæ¯é¡µçš„å®Œæ•´æå–æ–‡æœ¬
            "content_statistics": {},
            "potential_issues": [],
            "recommendations": []
        }
        
        try:
            with fitz.open(file_path) as doc:
                # ç¡®å®šåˆ†æé¡µé¢èŒƒå›´
                total_pages = len(doc)
                analyze_pages = total_pages if max_pages == 0 else min(max_pages, total_pages)
                
                print(f"æ–‡æ¡£æ€»é¡µæ•°: {total_pages}, åˆ†æé¡µæ•°: {analyze_pages}")
                
                # 1. åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
                results["file_info"] = self._analyze_file_info(doc, file_path)
                results["file_info"]["analyze_pages"] = analyze_pages
                results["file_info"]["total_pages"] = total_pages
                
                # 2. åŸºç¡€æå–æµ‹è¯•
                results["basic_extraction"] = self._test_basic_extraction(doc, analyze_pages)
                
                # 3. å¤šç§æå–æ–¹æ³•å¯¹æ¯”ï¼ˆä½¿ç”¨ç¬¬ä¸€é¡µä½œä¸ºæ ·æœ¬ï¼‰
                results["method_comparison"] = self._compare_extraction_methods(doc)
                
                # 4. é€é¡µè¯¦ç»†åˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰
                page_analysis, page_contents, extracted_texts = self._analyze_pages_with_content(doc, analyze_pages)
                results["page_analysis"] = page_analysis
                results["page_contents"] = page_contents
                results["extracted_texts"] = extracted_texts
                
                # 5. å†…å®¹ç»Ÿè®¡åˆ†æ
                results["content_statistics"] = self._calculate_content_statistics(results["page_analysis"])
                
                # 6. é—®é¢˜è¯Šæ–­
                results["potential_issues"] = self._diagnose_potential_issues(results)
                
                # 7. æ”¹è¿›å»ºè®®
                results["recommendations"] = self._generate_recommendations(results)
                
                # 8. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
                self._print_analysis_report(results)
                
                return results
                
        except Exception as e:
            logger.error(f"PDFåˆ†æå¤±è´¥: {str(e)}")
            print(f"âŒ PDFåˆ†æå¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_file_info(self, doc, file_path: str) -> Dict:
        """åˆ†ææ–‡ä»¶åŸºç¡€ä¿¡æ¯"""
        file_info = {
            "file_path": file_path,
            "file_size_mb": round(os.path.getsize(file_path) / (1024*1024), 2),
            "total_pages": len(doc),
            "is_password_protected": doc.needs_pass,
            "metadata": {
                "title": doc.metadata.get("title", ""),
                "author": doc.metadata.get("author", ""),
                "subject": doc.metadata.get("subject", ""),
                "keywords": doc.metadata.get("keywords", ""),
                "creator": doc.metadata.get("creator", ""),
                "producer": doc.metadata.get("producer", ""),
                "creation_date": doc.metadata.get("creationDate", ""),
                "modification_date": doc.metadata.get("modDate", "")
            }
        }
        
        print(f"\nğŸ“„ æ–‡ä»¶ä¿¡æ¯:")
        print(f"   æ–‡ä»¶å¤§å°: {file_info['file_size_mb']} MB")
        print(f"   æ€»é¡µæ•°: {file_info['total_pages']}")
        print(f"   æ˜¯å¦åŠ å¯†: {'æ˜¯' if file_info['is_password_protected'] else 'å¦'}")
        if file_info['metadata']['title']:
            print(f"   æ–‡æ¡£æ ‡é¢˜: {file_info['metadata']['title']}")
        
        return file_info
    
    def _test_basic_extraction(self, doc, max_pages: int) -> Dict:
        """æµ‹è¯•åŸºç¡€æ–‡æœ¬æå–"""
        print(f"\nğŸ” åŸºç¡€æå–æµ‹è¯•:")
        
        basic_results = {}
        analyze_pages = min(max_pages, len(doc))
        
        # æ–¹æ³•1: ç®€å•æ–‡æœ¬æå–
        try:
            simple_text = ""
            for page_num in range(analyze_pages):
                page = doc[page_num]
                simple_text += page.get_text()
            
            basic_results["simple_extraction"] = {
                "success": True,
                "total_chars": len(simple_text),
                "pages_analyzed": analyze_pages,
                "preview": simple_text[:200] + "..." if len(simple_text) > 200 else simple_text
            }
            print(f"   âœ… ç®€å•æå–: {len(simple_text)} å­—ç¬¦ ({analyze_pages}é¡µ)")
            
        except Exception as e:
            basic_results["simple_extraction"] = {"success": False, "error": str(e)}
            print(f"   âŒ ç®€å•æå–å¤±è´¥: {str(e)}")
        
        # æ–¹æ³•2: å¸ƒå±€ä¿æŒæå–
        try:
            layout_text = ""
            for page_num in range(analyze_pages):
                page = doc[page_num]
                layout_text += page.get_text("text")
            
            basic_results["layout_extraction"] = {
                "success": True,
                "total_chars": len(layout_text),
                "pages_analyzed": analyze_pages,
                "preview": layout_text[:200] + "..." if len(layout_text) > 200 else layout_text
            }
            print(f"   âœ… å¸ƒå±€æå–: {len(layout_text)} å­—ç¬¦ ({analyze_pages}é¡µ)")
            
        except Exception as e:
            basic_results["layout_extraction"] = {"success": False, "error": str(e)}
            print(f"   âŒ å¸ƒå±€æå–å¤±è´¥: {str(e)}")
        
        return basic_results
    
    def _compare_extraction_methods(self, doc) -> Dict:
        """å¯¹æ¯”ä¸åŒæå–æ–¹æ³•çš„æ•ˆæœ"""
        print(f"\nğŸ”¬ å¤šç§æå–æ–¹æ³•å¯¹æ¯”:")
        
        methods = {}
        
        # æµ‹è¯•æ‰€æœ‰é¡µé¢çš„ç¬¬ä¸€é¡µä½œä¸ºæ ·æœ¬
        if len(doc) > 0:
            sample_page = doc[0]
            
            # æ–¹æ³•1: get_text() - çº¯æ–‡æœ¬
            try:
                text1 = sample_page.get_text()
                methods["pure_text"] = {
                    "method": "get_text()",
                    "chars": len(text1),
                    "lines": text1.count('\n'),
                    "preview": text1[:150] + "..." if len(text1) > 150 else text1
                }
                line_count = text1.count('\n')
                print(f"   æ–¹æ³•1 - çº¯æ–‡æœ¬: {len(text1)} å­—ç¬¦, {line_count} è¡Œ")
            except Exception as e:
                methods["pure_text"] = {"error": str(e)}
            
            # æ–¹æ³•2: get_text("text") - å¸ƒå±€æ–‡æœ¬
            try:
                text2 = sample_page.get_text("text")
                methods["layout_text"] = {
                    "method": "get_text('text')",
                    "chars": len(text2),
                    "lines": text2.count('\n'),
                    "preview": text2[:150] + "..." if len(text2) > 150 else text2
                }
                line_count2 = text2.count('\n')
                print(f"   æ–¹æ³•2 - å¸ƒå±€æ–‡æœ¬: {len(text2)} å­—ç¬¦, {line_count2} è¡Œ")
            except Exception as e:
                methods["layout_text"] = {"error": str(e)}
            
            # æ–¹æ³•3: get_text("blocks") - æ–‡æœ¬å—
            try:
                blocks = sample_page.get_text("blocks")
                total_chars = sum(len(block[4]) for block in blocks if len(block) > 4)
                methods["blocks"] = {
                    "method": "get_text('blocks')",
                    "block_count": len(blocks),
                    "total_chars": total_chars,
                    "first_block": blocks[0][4][:100] + "..." if blocks and len(blocks[0]) > 4 else "æ— å†…å®¹"
                }
                print(f"   æ–¹æ³•3 - æ–‡æœ¬å—: {len(blocks)} å—, {total_chars} å­—ç¬¦")
            except Exception as e:
                methods["blocks"] = {"error": str(e)}
            
            # æ–¹æ³•4: get_text("dict") - ç»“æ„åŒ–æ•°æ®ï¼ˆå½“å‰æ–¹æ³•ï¼‰
            try:
                text_dict = sample_page.get_text("dict")
                block_count = len(text_dict.get("blocks", []))
                
                # ç»Ÿè®¡æ€»å­—ç¬¦æ•°
                total_chars = 0
                for block in text_dict.get("blocks", []):
                    for line in block.get("lines", []):
                        for span in line.get("spans", []):
                            total_chars += len(span.get("text", ""))
                
                methods["structured_dict"] = {
                    "method": "get_text('dict')",
                    "block_count": block_count,
                    "total_chars": total_chars,
                    "has_font_info": True
                }
                print(f"   æ–¹æ³•4 - ç»“æ„åŒ–å­—å…¸: {block_count} å—, {total_chars} å­—ç¬¦")
            except Exception as e:
                methods["structured_dict"] = {"error": str(e)}
            
            # æ–¹æ³•5: get_text("html") - HTMLæ ¼å¼
            try:
                html_text = sample_page.get_text("html")
                # ç§»é™¤HTMLæ ‡ç­¾æ¥è®¡ç®—çº¯æ–‡æœ¬é•¿åº¦
                import re
                clean_text = re.sub(r'<[^>]+>', '', html_text)
                methods["html"] = {
                    "method": "get_text('html')",
                    "html_length": len(html_text),
                    "text_length": len(clean_text),
                    "has_formatting": True
                }
                print(f"   æ–¹æ³•5 - HTMLæ ¼å¼: HTML {len(html_text)} å­—ç¬¦, çº¯æ–‡æœ¬ {len(clean_text)} å­—ç¬¦")
            except Exception as e:
                methods["html"] = {"error": str(e)}
        
        return methods
    
    def _analyze_pages_with_content(self, doc, max_pages: int) -> tuple[List[Dict], List[Dict], Dict]:
        """è¯¦ç»†åˆ†ææ¯ä¸€é¡µçš„å†…å®¹å¹¶ä¿å­˜å®Œæ•´æ–‡æœ¬"""
        print(f"\nğŸ“Š é€é¡µå†…å®¹åˆ†æ:")
        
        page_analyses = []
        page_contents = []  # è¯¦ç»†çš„é¡µé¢å†…å®¹ç»“æ„
        extracted_texts = {}  # æ¯é¡µçš„å®Œæ•´æå–æ–‡æœ¬
        
        analyze_pages = min(max_pages, len(doc))
        
        for page_num in range(analyze_pages):
            page = doc[page_num]
            
            analysis = {
                "page_number": page_num + 1,
                "extraction_methods": {},
                "content_types": {},
                "quality_metrics": {}
            }
            
            page_content = {
                "page_number": page_num + 1,
                "text_blocks": [],
                "images": [],
                "tables": [],
                "fonts_used": [],
                "bbox_info": {}
            }
            
            # æµ‹è¯•å¤šç§æå–æ–¹æ³•
            try:
                # 1. çº¯æ–‡æœ¬æå–
                simple_text = page.get_text()
                newline_count = simple_text.count('\n')
                analysis["extraction_methods"]["simple"] = {
                    "chars": len(simple_text),
                    "non_whitespace_chars": len(simple_text.replace(' ', '').replace('\n', '')),
                    "lines": newline_count
                }
                
                # ä¿å­˜å®Œæ•´æå–æ–‡æœ¬
                extracted_texts[f"page_{page_num + 1}"] = {
                    "simple_text": simple_text,
                    "simple_text_preview": simple_text[:500] + "..." if len(simple_text) > 500 else simple_text
                }
                
                # 2. ç»“æ„åŒ–æå–
                text_dict = page.get_text("dict")
                
                # ç»Ÿè®¡ç»“æ„åŒ–ä¿¡æ¯
                blocks = text_dict.get("blocks", [])
                text_blocks = [b for b in blocks if "lines" in b]
                image_blocks = [b for b in blocks if "lines" not in b]
                
                # è¯¦ç»†ä¿å­˜æ–‡æœ¬å—ä¿¡æ¯
                for i, block in enumerate(text_blocks):
                    block_info = {
                        "block_index": i,
                        "bbox": block.get("bbox", []),
                        "block_type": block.get("type", 0),
                        "lines": []
                    }
                    
                    block_text = ""
                    for line in block.get("lines", []):
                        line_info = {
                            "bbox": line.get("bbox", []),
                            "wmode": line.get("wmode", 0),
                            "dir": line.get("dir", []),
                            "spans": []
                        }
                        
                        line_text = ""
                        for span in line.get("spans", []):
                            span_info = {
                                "text": span.get("text", ""),
                                "font": span.get("font", ""),
                                "size": span.get("size", 12),
                                "flags": span.get("flags", 0),
                                "color": span.get("color", 0),
                                "bbox": span.get("bbox", [])
                            }
                            line_info["spans"].append(span_info)
                            line_text += span.get("text", "")
                        
                        line_info["text"] = line_text
                        block_info["lines"].append(line_info)
                        block_text += line_text + "\n"
                    
                    block_info["text"] = block_text.strip()
                    page_content["text_blocks"].append(block_info)
                
                # ä¿å­˜å›¾ç‰‡å—ä¿¡æ¯
                for i, img_block in enumerate(image_blocks):
                    img_info = {
                        "block_index": i,
                        "bbox": img_block.get("bbox", []),
                        "block_type": img_block.get("type", 1),
                        "width": img_block.get("width", 0),
                        "height": img_block.get("height", 0)
                    }
                    page_content["images"].append(img_info)
                
                # ç»Ÿè®¡å­—ä½“ä¿¡æ¯
                font_sizes = []
                font_names = []
                total_spans = 0
                
                for block in text_blocks:
                    for line in block.get("lines", []):
                        for span in line.get("spans", []):
                            total_spans += 1
                            font_size = span.get("size", 12)
                            font_name = span.get("font", "")
                            font_sizes.append(font_size)
                            font_names.append(font_name)
                
                unique_fonts = list(set(font_names)) if font_names else []
                page_content["fonts_used"] = [
                    {"font": font, "count": font_names.count(font)} 
                    for font in unique_fonts
                ]
                
                analysis["extraction_methods"]["structured"] = {
                    "total_blocks": len(blocks),
                    "text_blocks": len(text_blocks),
                    "image_blocks": len(image_blocks),
                    "total_spans": total_spans,
                    "unique_fonts": len(unique_fonts),
                    "font_size_range": [min(font_sizes), max(font_sizes)] if font_sizes else [0, 0]
                }
                
                # 3. å†…å®¹ç±»å‹åˆ†æ
                chinese_chars = len([c for c in simple_text if '\u4e00' <= c <= '\u9fff'])
                english_chars = len([c for c in simple_text if c.isalpha() and ord(c) < 128])
                digit_chars = len([c for c in simple_text if c.isdigit()])
                
                analysis["content_types"] = {
                    "chinese_chars": chinese_chars,
                    "english_chars": english_chars,
                    "digit_chars": digit_chars,
                    "total_printable": chinese_chars + english_chars + digit_chars,
                    "chinese_ratio": chinese_chars / max(len(simple_text), 1)
                }
                
                # 4. è´¨é‡æŒ‡æ ‡
                analysis["quality_metrics"] = {
                    "is_likely_scanned": analysis["content_types"]["total_printable"] < 50,
                    "has_structured_content": len(text_blocks) > 0,
                    "content_density": analysis["content_types"]["total_printable"] / max(len(simple_text), 1),
                    "extraction_success": len(simple_text) > 20
                }
                
                # 5. é¡µé¢è¾¹ç•Œä¿¡æ¯
                page_content["bbox_info"] = {
                    "page_width": page.rect.width,
                    "page_height": page.rect.height,
                    "rotation": page.rotation
                }
                
            except Exception as e:
                analysis["error"] = str(e)
                page_content["error"] = str(e)
            
            page_analyses.append(analysis)
            page_contents.append(page_content)
            
            # æ‰“å°é¡µé¢åˆ†æç»“æœ
            if "error" not in analysis:
                simple_chars = analysis["extraction_methods"]["simple"]["chars"]
                printable_chars = analysis["content_types"]["total_printable"]
                status = "âœ…" if analysis["quality_metrics"]["extraction_success"] else "âŒ"
                print(f"   ç¬¬{page_num + 1}é¡µ: {status} {simple_chars}å­—ç¬¦ ({printable_chars}æœ‰æ•ˆ)")
            else:
                print(f"   ç¬¬{page_num + 1}é¡µ: âŒ åˆ†æå¤±è´¥ - {analysis['error']}")
        
        if len(doc) > analyze_pages:
            print(f"   ... (å…±{len(doc)}é¡µï¼Œåˆ†æäº†å‰{analyze_pages}é¡µ)")
        
        return page_analyses, page_contents, extracted_texts
    
    def _calculate_content_statistics(self, page_analyses: List[Dict]) -> Dict:
        """è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_pages_analyzed": len(page_analyses),
            "successful_extractions": 0,
            "total_characters": 0,
            "total_printable_characters": 0,
            "empty_pages": 0,
            "low_content_pages": 0,
            "average_chars_per_page": 0,
            "content_quality_distribution": {"high": 0, "medium": 0, "low": 0}
        }
        
        for analysis in page_analyses:
            if "error" not in analysis and analysis["quality_metrics"]["extraction_success"]:
                stats["successful_extractions"] += 1
                
                chars = analysis["extraction_methods"]["simple"]["chars"]
                printable = analysis["content_types"]["total_printable"]
                
                stats["total_characters"] += chars
                stats["total_printable_characters"] += printable
                
                if printable == 0:
                    stats["empty_pages"] += 1
                elif printable < 50:
                    stats["low_content_pages"] += 1
                
                # è´¨é‡åˆ†ç±»
                density = analysis["quality_metrics"]["content_density"]
                if density > 0.3 and printable > 100:
                    stats["content_quality_distribution"]["high"] += 1
                elif density > 0.1 and printable > 20:
                    stats["content_quality_distribution"]["medium"] += 1
                else:
                    stats["content_quality_distribution"]["low"] += 1
        
        if stats["successful_extractions"] > 0:
            stats["average_chars_per_page"] = stats["total_characters"] / stats["successful_extractions"]
        
        return stats
    
    def _diagnose_potential_issues(self, results: Dict) -> List[str]:
        """è¯Šæ–­æ½œåœ¨é—®é¢˜"""
        issues = []
        
        stats = results["content_statistics"]
        
        # æ£€æŸ¥æ•´ä½“æå–æˆåŠŸç‡
        if stats["successful_extractions"] / max(stats["total_pages_analyzed"], 1) < 0.8:
            issues.append("âš ï¸  æå–æˆåŠŸç‡åä½ï¼Œå¯èƒ½å­˜åœ¨é¡µé¢æ ¼å¼é—®é¢˜")
        
        # æ£€æŸ¥ç©ºé¡µæ¯”ä¾‹
        if stats["empty_pages"] > 0:
            issues.append(f"âš ï¸  å‘ç° {stats['empty_pages']} ä¸ªç©ºé¡µï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆæˆ–å›¾ç‰‡é¡µ")
        
        # æ£€æŸ¥å†…å®¹å¯†åº¦
        if stats["total_printable_characters"] / max(stats["total_characters"], 1) < 0.1:
            issues.append("âš ï¸  æœ‰æ•ˆå­—ç¬¦æ¯”ä¾‹è¿‡ä½ï¼Œå¯èƒ½åŒ…å«å¤§é‡æ ¼å¼å­—ç¬¦æˆ–ç©ºç™½")
        
        # æ£€æŸ¥å¹³å‡é¡µé¢å†…å®¹
        if stats["average_chars_per_page"] < 200:
            issues.append("âš ï¸  å¹³å‡æ¯é¡µå­—ç¬¦æ•°è¿‡å°‘ï¼Œå¯èƒ½æ˜¯å›¾ç‰‡ä¸ºä¸»çš„æ–‡æ¡£")
        
        # æ£€æŸ¥è´¨é‡åˆ†å¸ƒ
        quality_dist = stats["content_quality_distribution"]
        if quality_dist["low"] > quality_dist["high"] + quality_dist["medium"]:
            issues.append("âš ï¸  ä½è´¨é‡é¡µé¢è¿‡å¤šï¼Œå»ºè®®æ£€æŸ¥æ–‡æ¡£ç±»å‹å’Œæå–ç­–ç•¥")
        
        # æ£€æŸ¥æ–¹æ³•å¯¹æ¯”
        methods = results.get("method_comparison", {})
        if "structured_dict" in methods and "pure_text" in methods:
            struct_chars = methods["structured_dict"].get("total_chars", 0)
            pure_chars = methods["pure_text"].get("chars", 0)
            
            if struct_chars < pure_chars * 0.8:
                issues.append("âš ï¸  ç»“æ„åŒ–æå–ä¸¢å¤±äº†è¾ƒå¤šå†…å®¹ï¼Œå»ºè®®ä¼˜åŒ–æå–é€»è¾‘")
        
        return issues
    
    def _generate_recommendations(self, results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        issues = results["potential_issues"]
        stats = results["content_statistics"]
        
        # åŸºäºé—®é¢˜ç”Ÿæˆå»ºè®®
        if any("ç©ºé¡µ" in issue for issue in issues):
            recommendations.append("ğŸ’¡ å»ºè®®å¢åŠ OCRå¤„ç†èƒ½åŠ›ï¼Œå¤„ç†æ‰«æç‰ˆPDF")
        
        if any("å­—ç¬¦æ•°è¿‡å°‘" in issue for issue in issues):
            recommendations.append("ğŸ’¡ å»ºè®®æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡ç±»æ–‡æ¡£ï¼Œè€ƒè™‘å›¾åƒæ–‡æœ¬è¯†åˆ«")
        
        if any("ç»“æ„åŒ–æå–ä¸¢å¤±" in issue for issue in issues):
            recommendations.append("ğŸ’¡ å»ºè®®ä¼˜åŒ–æ–‡æœ¬å—åˆå¹¶é€»è¾‘ï¼Œå‡å°‘å†…å®¹ä¸¢å¤±")
        
        if stats["content_quality_distribution"]["low"] > 3:
            recommendations.append("ğŸ’¡ å»ºè®®è°ƒæ•´è´¨é‡è¿‡æ»¤é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦è¿‡æ»¤")
        
        # é€šç”¨å»ºè®®
        if len(recommendations) == 0:
            recommendations.append("âœ… æ–‡æ¡£æå–è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¿æŒå½“å‰æå–ç­–ç•¥")
        
        recommendations.append("ğŸ’¡ å»ºè®®å®šæœŸä½¿ç”¨ä¸åŒç±»å‹PDFæ–‡æ¡£æµ‹è¯•æå–æ•ˆæœ")
        
        return recommendations
    
    def _print_analysis_report(self, results: Dict):
        """æ‰“å°è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ PDFå†…å®¹æå–åˆ†ææŠ¥å‘Š")
        print(f"{'='*60}")
        
        # æ–‡ä»¶æ¦‚å†µ
        file_info = results["file_info"]
        print(f"\nğŸ“„ æ–‡ä»¶æ¦‚å†µ:")
        print(f"   æ–‡ä»¶å¤§å°: {file_info['file_size_mb']} MB")
        print(f"   æ€»é¡µæ•°: {file_info['total_pages']}")
        print(f"   åˆ†æé¡µæ•°: {file_info.get('analyze_pages', 'æœªçŸ¥')}")
        
        # æå–ç»Ÿè®¡
        stats = results["content_statistics"]
        print(f"\nğŸ“Š æå–ç»Ÿè®¡:")
        print(f"   æˆåŠŸæå–é¡µæ•°: {stats['successful_extractions']}/{stats['total_pages_analyzed']}")
        print(f"   æ€»å­—ç¬¦æ•°: {stats['total_characters']:,}")
        print(f"   æœ‰æ•ˆå­—ç¬¦æ•°: {stats['total_printable_characters']:,}")
        print(f"   å¹³å‡æ¯é¡µå­—ç¬¦: {stats['average_chars_per_page']:.0f}")
        print(f"   ç©ºé¡µæ•°: {stats['empty_pages']}")
        
        # è´¨é‡åˆ†å¸ƒ
        quality = stats["content_quality_distribution"]
        print(f"\nğŸ¯ å†…å®¹è´¨é‡åˆ†å¸ƒ:")
        print(f"   é«˜è´¨é‡é¡µé¢: {quality['high']}")
        print(f"   ä¸­ç­‰è´¨é‡é¡µé¢: {quality['medium']}")
        print(f"   ä½è´¨é‡é¡µé¢: {quality['low']}")
        
        # é—®é¢˜è¯Šæ–­
        issues = results["potential_issues"]
        if issues:
            print(f"\nâš ï¸  å‘ç°çš„é—®é¢˜:")
            for issue in issues:
                print(f"   {issue}")
        
        # æ”¹è¿›å»ºè®®
        recommendations = results["recommendations"]
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for rec in recommendations:
            print(f"   {rec}")
        
        print(f"\n{'='*60}")
        print(f"åˆ†æå®Œæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}")
    
    def save_detailed_report(self, results: Dict, output_path: str):
        """ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            # æ·»åŠ åˆ†ææ—¶é—´
            results["analysis_timestamp"] = datetime.now().isoformat()
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            
            # é¢å¤–ä¿å­˜ä¸€ä¸ªçº¯æ–‡æœ¬ç‰ˆæœ¬ï¼Œæ–¹ä¾¿æŸ¥çœ‹æå–çš„å†…å®¹
            text_output_path = output_path.replace('.json', '_extracted_text.txt')
            self._save_extracted_text_file(results, text_output_path)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def _save_extracted_text_file(self, results: Dict, output_path: str):
        """ä¿å­˜æå–çš„çº¯æ–‡æœ¬å†…å®¹åˆ°æ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("="*80 + "\n")
                f.write("PDFæå–æ–‡æœ¬å†…å®¹è¯¦ç»†æŠ¥å‘Š\n")
                f.write("="*80 + "\n\n")
                
                # åŸºæœ¬ä¿¡æ¯
                file_info = results.get("file_info", {})
                f.write(f"æ–‡ä»¶è·¯å¾„: {file_info.get('file_path', 'æœªçŸ¥')}\n")
                f.write(f"æ–‡ä»¶å¤§å°: {file_info.get('file_size_mb', 0)} MB\n")
                f.write(f"æ€»é¡µæ•°: {file_info.get('total_pages', 0)}\n")
                f.write(f"åˆ†æé¡µæ•°: {file_info.get('analyze_pages', 0)}\n\n")
                
                # é€é¡µå†…å®¹
                extracted_texts = results.get("extracted_texts", {})
                page_contents = results.get("page_contents", [])
                
                for page_content in page_contents:
                    page_num = page_content["page_number"]
                    f.write(f"\n{'='*60}\n")
                    f.write(f"ç¬¬ {page_num} é¡µå†…å®¹\n")
                    f.write(f"{'='*60}\n")
                    
                    # é¡µé¢åŸºæœ¬ä¿¡æ¯
                    if "bbox_info" in page_content:
                        bbox = page_content["bbox_info"]
                        f.write(f"é¡µé¢å°ºå¯¸: {bbox.get('page_width', 0):.1f} x {bbox.get('page_height', 0):.1f}\n")
                        f.write(f"é¡µé¢æ—‹è½¬: {bbox.get('rotation', 0)}åº¦\n")
                    
                    # æ–‡æœ¬å—ä¿¡æ¯
                    text_blocks = page_content.get("text_blocks", [])
                    f.write(f"æ–‡æœ¬å—æ•°é‡: {len(text_blocks)}\n")
                    
                    # å›¾ç‰‡ä¿¡æ¯
                    images = page_content.get("images", [])
                    f.write(f"å›¾ç‰‡å—æ•°é‡: {len(images)}\n")
                    
                    # å­—ä½“ä¿¡æ¯
                    fonts = page_content.get("fonts_used", [])
                    if fonts:
                        f.write(f"ä½¿ç”¨å­—ä½“: {', '.join([font['font'] for font in fonts[:5]])}\n")
                    
                    f.write(f"\n--- æå–çš„å®Œæ•´æ–‡æœ¬ ---\n")
                    
                    # æå–çš„æ–‡æœ¬å†…å®¹
                    page_key = f"page_{page_num}"
                    if page_key in extracted_texts:
                        simple_text = extracted_texts[page_key].get("simple_text", "")
                        if simple_text.strip():
                            f.write(simple_text)
                            f.write(f"\n\n--- å­—ç¬¦ç»Ÿè®¡ ---\n")
                            f.write(f"æ€»å­—ç¬¦æ•°: {len(simple_text)}\n")
                            
                            # ä¿®å¤ï¼šå…ˆè®¡ç®—éç©ºç™½å­—ç¬¦æ•°ï¼Œå†å†™å…¥
                            non_whitespace_count = len(simple_text.replace(' ', '').replace('\n', '').replace('\t', ''))
                            f.write(f"éç©ºç™½å­—ç¬¦æ•°: {non_whitespace_count}\n")
                            
                            # ä¿®å¤ï¼šå…ˆè®¡ç®—è¡Œæ•°ï¼Œå†å†™å…¥
                            line_count = simple_text.count('\n') + 1
                            f.write(f"è¡Œæ•°: {line_count}\n")
                        else:
                            f.write("ã€æ­¤é¡µæ— æ–‡æœ¬å†…å®¹æˆ–æå–å¤±è´¥ã€‘\n")
                    else:
                        f.write("ã€æå–æ•°æ®ç¼ºå¤±ã€‘\n")
                    
                    # è¯¦ç»†çš„æ–‡æœ¬å—å†…å®¹
                    if text_blocks:
                        f.write(f"\n--- ç»“æ„åŒ–æ–‡æœ¬å—è¯¦æƒ… ---\n")
                        for i, block in enumerate(text_blocks):
                            f.write(f"\næ–‡æœ¬å— {i+1}:\n")
                            f.write(f"ä½ç½®: {block.get('bbox', [])}\n")
                            f.write(f"å†…å®¹: {block.get('text', '').strip()}\n")
                            f.write("-" * 40 + "\n")
                
                # æ€»ç»“ä¿¡æ¯
                f.write(f"\n{'='*80}\n")
                f.write(f"æå–æ€»ç»“\n")
                f.write(f"{'='*80}\n")
                
                stats = results.get("content_statistics", {})
                f.write(f"æˆåŠŸæå–é¡µæ•°: {stats.get('successful_extractions', 0)}/{stats.get('total_pages_analyzed', 0)}\n")
                f.write(f"æ€»å­—ç¬¦æ•°: {stats.get('total_characters', 0):,}\n")
                f.write(f"æœ‰æ•ˆå­—ç¬¦æ•°: {stats.get('total_printable_characters', 0):,}\n")
                f.write(f"å¹³å‡æ¯é¡µå­—ç¬¦æ•°: {stats.get('average_chars_per_page', 0):.0f}\n")
                
                # é—®é¢˜å’Œå»ºè®®
                issues = results.get("potential_issues", [])
                if issues:
                    f.write(f"\nå‘ç°çš„é—®é¢˜:\n")
                    for issue in issues:
                        f.write(f"- {issue}\n")
                
                recommendations = results.get("recommendations", [])
                if recommendations:
                    f.write(f"\næ”¹è¿›å»ºè®®:\n")
                    for rec in recommendations:
                        f.write(f"- {rec}\n")
            
            print(f"ğŸ’¾ æå–æ–‡æœ¬å·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")

# å¿«é€Ÿä½¿ç”¨å‡½æ•°
def analyze_pdf_extraction(pdf_path: str, save_report: bool = True, max_pages: int = 20):
    """å¿«é€Ÿåˆ†æPDFæå–æ•ˆæœçš„å…¥å£å‡½æ•°
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        save_report: æ˜¯å¦ä¿å­˜æŠ¥å‘Š
        max_pages: æœ€å¤§åˆ†æé¡µæ•°ï¼Œ0è¡¨ç¤ºåˆ†ææ‰€æœ‰é¡µé¢
    """
    analyzer = PDFDebugAnalyzer()
    results = analyzer.comprehensive_analysis(pdf_path, max_pages)
    
    if save_report and "error" not in results:
        report_path = pdf_path.replace('.pdf', '_detailed_extraction_analysis.json')
        analyzer.save_detailed_report(results, report_path)
    
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        pdf_path = sys.argv[1]
        max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 20
        analyze_pdf_extraction(pdf_path, max_pages=max_pages)
    else:
        print("ç”¨æ³•: python pdf_debug_analyzer.py <PDFæ–‡ä»¶è·¯å¾„> [æœ€å¤§åˆ†æé¡µæ•°]")
        print("ç¤ºä¾‹: python pdf_debug_analyzer.py document.pdf 20")
        print("      python pdf_debug_analyzer.py document.pdf 0  # åˆ†ææ‰€æœ‰é¡µé¢") 